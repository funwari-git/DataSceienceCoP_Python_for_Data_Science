{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da62fee",
   "metadata": {},
   "source": [
    "<table width=100%; style=\"background-color:#caf0fa\";>\n",
    "    <tr style=\"background-color:#caf0fa\">\n",
    "        <td>\n",
    "            <h1 style=\"text-align:right\">\n",
    "                Python for Data Science Training - Week 6\n",
    "            </h1>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../img/jica-logo.png\" alt = \"JICA Training\" style = \"width: 100px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "# Today's Contents\n",
    "## Machine Learning\n",
    "\n",
    "---\n",
    "Before starting the session, you need to install [scikit-learn](https://scikit-learn.org/stable/index.html), the most famous machine learning library in Python. Launch your command prompt with `cmd`, then run this command:\n",
    "```pyton\n",
    "conda install -c anaconda scikit-learn\n",
    "```\n",
    "\n",
    "There are three Machine Learning tasks - **regression task**, **classification task**, and **clustering task**. Today we will conduct regression and classification tasks which both applies supervised machine learning, and leaving the clustering task which is primarily done with unsupervised machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf48b89",
   "metadata": {},
   "source": [
    "# 1. Regression Problem\n",
    "A regression problem is to take a continuous variable as a dependent vairable, for example, price and temperature, to predict the value given provided independent variable(s). The method applied is ordinary least square (OLS), which attempts to minimize the loss between the actuval value of the observed dependent variable and the predicted value by the OLS.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/be/Normdist_regression.png\">\n",
    "\n",
    "source:[wikipedia](https://en.wikipedia.org/wiki/Regression_analysis)  \n",
    "\n",
    "\n",
    "We will estimate the heating and cooling load based on the given properties.  \n",
    "**Heating load** is the amount of heat energy that would need to be added to a space to maintain the temperature.  \n",
    "**Cooling load** is the amount o heat energy that would need to be removed from a space to maintain the temperature.  \n",
    "They, collectively called as \"thermal loads\", take into account:\n",
    "- the dwelling's construction and insulation; including floors, walls, ceilings and roof; and\n",
    "- the dwelling's glazing and skylights; based on size, performance, shading and overshadowing.\n",
    "\n",
    "Lower thermal loads indicate that the dwelling will require less heating and cooling to maintain comfortable conditions.  \n",
    "(Heating and cooling loads, [BASIX](https://basix.nsw.gov.au/iframe/thermal-help/heating-and-cooling-loads.html))\n",
    "\n",
    "We will use the dataset provided by [UCI ML Repository](\n",
    "https://archive.ics.uci.edu/ml/datasets/Energy+efficiency#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "# ignore user error message\n",
    "import warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c296c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "df = pd.read_csv('data/energy_efficiency.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea3a166",
   "metadata": {},
   "source": [
    "According to the [documentation](https://archive.ics.uci.edu/ml/datasets/Energy+efficiency#), here is the definition  of the variables. Target variables are `y1` and `y2`, and the rest of the variables are the predictors.\n",
    "\n",
    "|feature name|variable|\n",
    "|---|---|\n",
    "|X1|Relative Compactness|\n",
    "|X2|Surface Area|\n",
    "|X3|Wall Area|\n",
    "|X4|Roof Area|\n",
    "|X5|Overall Height|\n",
    "|X6|Orientation|\n",
    "|X7|Glazing Area|\n",
    "|X8|Glazing Area Distribution|\n",
    "|y1|Heating Load|\n",
    "|y2|Cooling Load|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871bf98f",
   "metadata": {},
   "source": [
    "Since the feature names are not intuitive, we'll use variable names rather than feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cba9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names\n",
    "column_name_dict = {\n",
    "'X1':'Relative_Compactness',\n",
    "'X2':'Surface_Area',\n",
    "'X3':'Wall_Area',\n",
    "'X4':'Roof_Area',\n",
    "'X5':'Overall_Height',\n",
    "'X6':'Orientation',\n",
    "'X7':'Glazing_Area',\n",
    "'X8':'Glazing_Area_Distribution',\n",
    "'Y1':'Y1_Heating_Load',\n",
    "'Y2':'Y2_Cooling_Load'}\n",
    "\n",
    "df = df.rename(columns = column_name_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba61282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79676bc",
   "metadata": {},
   "source": [
    "The data is well interpreted and no need to conduct a processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00938e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3989139a",
   "metadata": {},
   "source": [
    "From this statistics, we understand that:\n",
    "- the sample size is 768.\n",
    "- Our target variables (Y1 and Y2) are both continuous variables.\n",
    "- Our input variables are similarly continous variables. (`Glazing_Area_Distribution` may be discrete, but we can intepret this as a continuous variable.)\n",
    "\n",
    "Let's review Pearson's correlation coefficient to find out if there are unrelated variables to predict the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking into Pearson's correlation coefficient \n",
    "df_corr = df.corr()\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17225289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's simplify the table by taking dependent variables as columns\n",
    "df_corr_targets = df_corr[['Y1_Heating_Load', 'Y2_Cooling_Load']].iloc[:-2]\n",
    "df_corr_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation\n",
    "df_corr_targets.plot(style = '.', figsize = (7, 5))\n",
    "plt.axhline(y = 0, linestyle = '--', color = 'k')\n",
    "plt.xticks(rotation = 90, fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.xlabel('Predictors', fontsize = 12)\n",
    "plt.ylabel('Correlation', fontsize = 12)\n",
    "plt.title('Pearson Correlation Coefficient', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e3ee2",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "Simply put, machine learning is to predict a dependent variable based on a set of input data. The following procedure is applied.\n",
    "1. Split data into X (input) and y (output).\n",
    "2. Split X and y into training and test data. (Normally, 80-20 or 70-30.)\n",
    "3. Rescaling X data.\n",
    "4. Build the model (fit -> evaluate)\n",
    "5. Predict the unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a952c9",
   "metadata": {},
   "source": [
    "## 1. Split data into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing dataset. We have two `y data`, we'll create y1 and y2.\n",
    "y1 = df['Y1_Heating_Load']\n",
    "y2 = df['Y2_Cooling_Load']\n",
    "X = df.drop(columns = ['Y1_Heating_Load', 'Y2_Cooling_Load'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Y1 data: ', y1.values[:5])\n",
    "print('Y2 data: ', y2.values[:5])\n",
    "print('X data: ', X.values[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375a2609",
   "metadata": {},
   "source": [
    "## 2. Split X and y into training and test data\n",
    "\n",
    "We will split our dataset into 70 percent of training data and 30 percent of test data. This can be quickly done with `train_test_split` under `sklearn.model_selection`. `train_test_split` takes arguments **X data**, **y data**, and **test_size** given as a percentage. Optionally we can add **random_state** to allow replication and **shuffle** to enable random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80bcf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `train_test_split` randomly select data based on the given test_size.\n",
    "X_train, X_test, y1_train, y1_test = train_test_split(X, y1, test_size = .3, random_state = 1234, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30efb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similary we can create y2 test data.\n",
    "X_train, X_test, y2_train, y2_test = train_test_split(X, y2, test_size = .3, random_state = 1234, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While it's not normal, we can split both y1 and y2 with only one command.\n",
    "X_train, X_test, y1_train, y1_test, y2_train, y2_test = train_test_split(X, y1, y2, test_size = .3,\n",
    "                                                                         random_state = 1234, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eebefc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can quickly inspect each data size.\n",
    "for data in [X_train, X_test, y1_train, y1_test, y2_train, y2_test]:\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941cfed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [X_train, X_test, y1_train, y1_test, y2_train, y2_test]:\n",
    "    print(data.values[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985722b",
   "metadata": {},
   "source": [
    "## 3. Rescaling X data.\n",
    "Machine learning does not consider nuances between variables, we need to tell a machine that the data needs to be equally treated. Since the machine considers a higher value is high and a lower value is low, we need to normalize data to explicitly inform the machine to treat the data equally.\n",
    "\n",
    "We'll standardize the data by taking mean 0 and standard deviation is 1. Mathematically put, scaled data is\n",
    "\n",
    "$$ z = \\frac{(x - u)}{s} $$\n",
    "\n",
    ", where *u* is the mean of the samples and *s* is the standard deviation of the samples.\n",
    "\n",
    "Rescaling is conducted with:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "````\n",
    "We'll initialize the `StandardScaler`, then fit and transform the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd9c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's quickly review X_train data\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also inspect test data\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Compute the mean and std based on X_train data.\n",
    "scaler = ss.fit(X_train)\n",
    "\n",
    "# Sdandardize X_train data\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# Standardize X_test data\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train transformed: ', X_train[0])\n",
    "print('X_test transformed: ', X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f78cb",
   "metadata": {},
   "source": [
    "## 4. Build the model (fit -> evaluate -> predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Initializing the model\n",
    "lr1 = LinearRegression()\n",
    "lr2 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Y1 Data\n",
    "# Fitting data to the model\n",
    "lr1.fit(X_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the train score\n",
    "lr_y1_train_score = lr1.score(X_train, y1_train)\n",
    "\n",
    "# Evaluate the test score\n",
    "lr_y1_test_score = lr1.score(X_test, y1_test)\n",
    "\n",
    "print('Training score on y1: ', lr_y1_train_score)\n",
    "print('Test score on y1: ', lr_y1_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values\n",
    "lr_y1_predictions = lr1.predict(X_test)\n",
    "\n",
    "print('ML results on predicting Heating Load (Y1)\\n')\n",
    "for y1, y1_hat in zip(y1_test[:10], lr_y1_predictions[:10]):\n",
    "    print('Actual: {}\\t| Predicted: {}'.format(round(y1, 3), round(y1_hat, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f3a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Y2 Data\n",
    "# Fitting data to the model\n",
    "lr2.fit(X_train, y2_train)\n",
    "\n",
    "# Evaluate the train score\n",
    "lr_y2_train_score = lr2.score(X_train, y2_train)\n",
    "\n",
    "# Evaluate the test score\n",
    "lr_y2_test_score = lr2.score(X_test, y2_test)\n",
    "\n",
    "# Predict values\n",
    "lr_y2_predictions = lr2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd677154",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ML results on predicting Cooling Load (Y2)\\n')\n",
    "for y2, y2_hat in zip(y2_test[:10], lr_y2_predictions[:10]):\n",
    "    print('Actual: {}\\t| Predicted: {}'.format(round(y2, 3), round(y2_hat, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing data\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "\n",
    "# scatter of actual and predicted\n",
    "ax1.scatter(y1_test, lr_y1_predictions, s = 8, marker = '.')\n",
    "# draw a diagonal line\n",
    "ax1.plot([0, max(max(y1_test), max(lr_y1_predictions))], [0, max(max(y1_test), max(lr_y1_predictions))], '--', color = 'red')\n",
    "# add ylabel\n",
    "ax1.set_ylabel('Predicted')\n",
    "# add xlabel\n",
    "ax1.set_xlabel('Actual')\n",
    "# add title\n",
    "ax1.set_title('Predicted results - y1 outcome | score={}'.format(round(lr_y1_test_score, 2)))\n",
    "\n",
    "ax2.scatter(y2_test, lr_y2_predictions, s = 8, marker = '.')\n",
    "ax2.plot([0, max(max(y2_test), max(lr_y2_predictions))], [0, max(max(y2_test), max(lr_y2_predictions))], '--', color = 'red')\n",
    "ax2.set_ylabel('Predicted')\n",
    "ax2.set_xlabel('Actual')\n",
    "ax2.set_title('Predicted results - y2 outcome | score={}'.format(round(lr_y2_test_score, 2)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8cc05a",
   "metadata": {},
   "source": [
    "We can retrive our intercept and coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e73767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficients: ', lr2.coef_)\n",
    "print('\\nIntercept: ', lr2.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefcbccc",
   "metadata": {},
   "source": [
    "From the above numbers, we can construct the formula as:\n",
    "\n",
    "$$\n",
    "y_(hat) = 24.38189944134079 + -7.90034619*b1 + -4.20952194*b2 + 0.01555827*b3 + -4.13528677*b4 + 7.41827072*b5 + 0.12330692*b6 + 1.94546433*b7 + 0.19784801*b8\n",
    "$$\n",
    "\n",
    "We can predict the value based on arbitrary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary values\n",
    "b_values = [-1.53852577,  1.43804518,  1.91398512,  0.94193515,  1.02069834, 1.30082467,  1.23012947,  1.41565359]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a739cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manuarlly predict the value\n",
    "b_estimated = 24.38189944134079 + -7.90034619*b_values[0] +\\\n",
    "                -4.20952194*b_values[1] + 0.01555827*b_values[2] +\\\n",
    "                -4.13528677*b_values[3] + 7.41827072*b_values[4] + \\\n",
    "                0.12330692*b_values[5] + 1.94546433*b_values[6] + 0.19784801*b_values[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted b on y2 outcome is: {:.2f}'.format(b_estimated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550ac31",
   "metadata": {},
   "source": [
    "## 5. Predict the unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d76a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346399d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacc780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random values\n",
    "big_house = [0.60, 800, 400, 220, 7, 5, 0.4, 5]\n",
    "small_house = [0.99, 500, 250, 1, 4, 0, 0, 0]\n",
    "strange_house = [0.60, 600, 300, 150, 10, 5, 0.4, 5]\n",
    "sample_dict = {}\n",
    "sample_dict['Big'] = big_house\n",
    "sample_dict['Small'] = small_house\n",
    "sample_dict['Strange'] = strange_house\n",
    "sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28233a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataframe\n",
    "sample_data = pd.DataFrame.from_dict(sample_dict, orient = 'index', columns = X.columns)\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d61620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sdandardize sample data\n",
    "sample_data_scaled = scaler.transform(sample_data)\n",
    "sample_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc769d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict heating and cooling values\n",
    "sample_y1_heating_load_hat = lr1.predict(sample_data_scaled)\n",
    "sample_y2_cooling_load_hat = lr2.predict(sample_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Heating performance - Big house = {:.2f} | Small house = {:.2f} | Strange house = {:.2f}'.format(\n",
    "                                                        sample_y1_heating_load_hat[0],\n",
    "                                                        sample_y1_heating_load_hat[1],\n",
    "                                                        sample_y1_heating_load_hat[2],\n",
    "))\n",
    "\n",
    "print('Cooling performance - Big house = {:.2f} | Small house = {:.2f} | Strange house = {:.2f}'.format(\n",
    "                                                        sample_y2_cooling_load_hat[0],\n",
    "                                                        sample_y2_cooling_load_hat[1],\n",
    "                                                        sample_y2_cooling_load_hat[2],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b973da",
   "metadata": {},
   "source": [
    "Looks like a small house performs the best, but do you want to live in such a house??   haha\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6da585",
   "metadata": {},
   "source": [
    "# Classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a07c54",
   "metadata": {},
   "source": [
    "We will predict the students' knowledge level about electrical machines. The knowledge level is classisifed into 4 categories: Very Low, Low, Middle, and High. We have five predictors to project the likely knowledge category.\n",
    "\n",
    "Classification task is to identify a set of categories an observation belongs to. An example of the classification task is depected below.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b5/Svm_separating_hyperplanes_%28SVG%29.svg\" width=600px>\n",
    "\n",
    "source: [wikipedia](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
    "\n",
    "\n",
    "\n",
    "We'll use the dataset from [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247f45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knowledge_train = pd.read_excel('data/User_Knowledge_Modeling_Data_Set.xls', sheet_name = 'Training_Data')\n",
    "df_knowledge_test  = pd.read_excel('data/User_Knowledge_Modeling_Data_Set.xls', sheet_name = 'Test_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d380b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knowledge_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87533670",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knowledge_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb32cb",
   "metadata": {},
   "source": [
    "According to this [documentation](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling#), the dataset is given as below.\n",
    "\n",
    "| feature name | description | input/output |\n",
    "|---|---|---|\n",
    "|STG| The degree of study time for goal object materails| input value|\n",
    "|SCG| The degree of repetition number of user for goal object materails| input value|\n",
    "|STR| The degree of study time of user for related objects with goal object| input value|\n",
    "|LPR| The exam performance of user for related objects with goal object| input value|\n",
    "|PEG| The exam performance of user for goal objects| input value|\n",
    "|UNS| The knowledge level of user| target value|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print column name\n",
    "print(df_knowledge_train.columns)\n",
    "print(df_knowledge_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns\n",
    "df_knowledge_train = df_knowledge_train[['STG', 'SCG', 'STR', 'LPR', 'PEG', ' UNS']]\n",
    "df_knowledge_test = df_knowledge_test[['STG', 'SCG', 'STR', 'LPR', 'PEG', ' UNS']]\n",
    "\n",
    "# Since there is a leading space in front of UNS, remove the space.\n",
    "df_knowledge_train = df_knowledge_train.rename(columns = {' UNS':'UNS'})\n",
    "df_knowledge_test = df_knowledge_test.rename(columns = {' UNS':'UNS'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "df_knowledge_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee186f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values\n",
    "df_knowledge_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b79906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check statistics\n",
    "df_knowledge_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e353c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string item to numeric ones\n",
    "print(df_knowledge_train['UNS'].unique())\n",
    "print(df_knowledge_test['UNS'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to replace string values of the column `UNS`\n",
    "replace_values_dict = {\n",
    "    'very_low': 1,\n",
    "    'Very Low':1,\n",
    "    'Low':2,\n",
    "    'Middle':3,\n",
    "    'High':4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the value to the above dictionary.\n",
    "df_knowledge_train['UNS'] = df_knowledge_train['UNS'].replace(replace_values_dict)\n",
    "df_knowledge_test['UNS'] = df_knowledge_test['UNS'].replace(replace_values_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a3ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if converted\n",
    "print(df_knowledge_train['UNS'].unique())\n",
    "print(df_knowledge_test['UNS'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0755f4",
   "metadata": {},
   "source": [
    "Since the data is pre-scaled within 0 and 1, we don't apply normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81cea7c",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f969be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing x and y data\n",
    "y_train = df_knowledge_train['UNS']\n",
    "X_train = df_knowledge_train.drop(columns = ['UNS'])\n",
    "\n",
    "y_test = df_knowledge_test['UNS']\n",
    "X_test = df_knowledge_test.drop(columns = ['UNS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c4d54",
   "metadata": {},
   "source": [
    "We will test three classifiers: K Nearest Neighbor, Support Vector Machine, and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f0780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a0013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct the classification task with three classifiers\n",
    "results_dict = {}\n",
    "for classifier, name in zip([KNeighborsClassifier, SVC, RandomForestClassifier], ['KNN', 'SVM', 'RFC']):\n",
    "    classifier_init = classifier()\n",
    "    classifier_init.fit(X_train, y_train)\n",
    "    predictions = classifier_init.predict(X_test)\n",
    "    results_dict[name] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look into the resulting dictionary\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c7e68",
   "metadata": {},
   "source": [
    "## Evaluate the results\n",
    "It is important to carefully review various results in classification. Key parameters are precision, recall and f1-score.\n",
    "- **Precision** is the fraction of relevant instances among the retrieved instances\n",
    "$$ Precision = \\frac{TP}{(TP + FP)} $$\n",
    "- **Recall** is the fraction of relevant instances that were retrieved.\n",
    "$$ Recall = \\frac{TP}{(TP + FN)} $$\n",
    "- **F1 score** is the harmonic mean of precision and recall:\n",
    "$$ F1 = \\frac{2TP}{(2TP + FP + FN)} $$\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\">\n",
    "\n",
    "source: [wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed59714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to report the results\n",
    "def evaluate_result(classifier):\n",
    "    pred = results_dict.get(classifier)\n",
    "    print('-----{}-----'.format(classifier))\n",
    "    \n",
    "    # Overall accuracy\n",
    "    print('- Overall accuracy score: {:.2f}'.format(accuracy_score(y_test, pred)))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print('\\n- Confusion Matrix -')\n",
    "    print(confusion_matrix(y_test, pred))\n",
    "    \n",
    "    # Classification report\n",
    "    print('\\n- Classification Report- ')\n",
    "    print(classification_report(y_test, pred, target_names = ['Very Low','Low','Middle','High']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e9b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_result('KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39764811",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_result('SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3960884",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_result('RFC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492083bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba320565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is to create a config file.\n",
    "# Hiding this cell for authority\n",
    "\n",
    "# Hiding celll from https://gist.github.com/Zsailer/5d1f4e357c78409dd9a5a4e5c61be552\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "# Taken from https://stackoverflow.com/questions/31517194/how-to-hide-one-specific-cell-input-or-output-in-ipython-notebook\n",
    "tag = HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "    if (code_show){\n",
    "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "    } else {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "    }\n",
    "    code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "Creating requirements.txt file. To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>.''')\n",
    "display(tag)\n",
    "\n",
    "############### Write code below ##################\n",
    "# Config file to freeze packages in a notebook\n",
    "# from https://stackoverflow.com/questions/40428931/package-for-listing-version-of-packages-used-in-a-jupyter-notebook\n",
    "import pkg_resources\n",
    "import types\n",
    "\n",
    "def get_requirements():\n",
    "    def get_imports():\n",
    "        for name, val in globals().items():\n",
    "            if isinstance(val, types.ModuleType):\n",
    "                # Split ensures you get root package, \n",
    "                # not just imported function\n",
    "                name = val.__name__.split(\".\")[0]\n",
    "\n",
    "            elif isinstance(val, type):\n",
    "                name = val.__module__.split(\".\")[0]\n",
    "\n",
    "            # Some packages are weird and have different\n",
    "            # imported names vs. system/pip names. Unfortunately,\n",
    "            # there is no systematic way to get pip names from\n",
    "            # a package's imported name. You'll have to add\n",
    "            # exceptions to this list manually!\n",
    "            poorly_named_packages = {\n",
    "                \"PIL\": \"Pillow\",\n",
    "                \"sklearn\": \"scikit-learn\"\n",
    "            }\n",
    "            if name in poorly_named_packages.keys():\n",
    "                name = poorly_named_packages[name]\n",
    "\n",
    "            yield name\n",
    "    imports = list(set(get_imports()))\n",
    "\n",
    "    # The only way I found to get the version of the root package\n",
    "    # from only the name of the package is to cross-check the names \n",
    "    # of installed packages vs. imported packages\n",
    "    requirements = []\n",
    "    for m in pkg_resources.working_set:\n",
    "        if m.project_name in imports and m.project_name!=\"pip\":\n",
    "            requirements.append((m.project_name, m.version))\n",
    "\n",
    "    \n",
    "    with open(\"requirements.txt\", \"w\") as f:\n",
    "        print('Create \"requirements.txt\"')\n",
    "        for r in requirements:\n",
    "            string = r[0] + '==' + r[1] + '\\n'\n",
    "            f.write(string)\n",
    "            print(\"\\t{}=={}\".format(*r))\n",
    "    print('\"requirements.txt\" was created.')\n",
    "        \n",
    "get_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488e578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
